import argparse
import time
import warnings
import os
import sys
import pickle
import numpy as np
import pandas as pd
import shap

from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

import optuna

# ĞŸÑƒÑ‚Ğ¸ Ğº ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼
sys.path.append(r"C:\Users\Denis\credit_scoring\src\app\utils")
sys.path.append(r"C:\Users\Denis\credit_scoring\src\config")
sys.path.append(r"C:\Users\Denis\credit_scoring\src\app\modelling\features")

from db_config import DB_ARGS
from db_handler import PostgresDBHandler

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", None)
pd.set_option("display.float_format", "{:.2f}".format)
np.set_printoptions(suppress=True, precision=2)

RANDOM_STATE = 42

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_and_merge_data(application_file: str, bureau_balance_file: str):
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CSV Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾ sk_id_curr,
    Ğ´ĞµĞ»Ğ¸Ñ‚ Ğ½Ğ° train/test, Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¸.
    """
    print("[INFO] Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")

    # ĞŸĞ¾Ğ´Ğ³Ñ€ÑƒĞ·ĞºĞ° CSV-Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
    df = pd.read_csv(application_file)
    bureau_balance = pd.read_csv(bureau_balance_file)

    # Ğ”ĞµĞ»Ğ¸Ğ¼ Ğ½Ğ° train/test Ğ¿Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ target
    test = df[df["target"].isnull()].copy()
    train = df[df["target"].notnull()].copy()

    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ´Ğ¾Ğ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² > 15%
    nan_thresh = 0.15
    missing_pct = train.isnull().mean()
    keep_cols = missing_pct[missing_pct <= nan_thresh].index.tolist()

    train = train[keep_cols]
    test = test[keep_cols]

    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ bureau_balance
    train = train.merge(bureau_balance, on="sk_id_curr", how="left")
    test = test.merge(bureau_balance, on="sk_id_curr", how="left")

    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¸ -1
    train = train.fillna(-1)
    test["target"] = test["target"].fillna(999)
    test = test.fillna(-1)
    test["target"] = test["target"].replace(999, np.nan)

    return train, test


def preprocess_data(train: pd.DataFrame):
    """
    Ğ”ĞµĞ»Ğ¸Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼ Ğ½Ğ° X Ğ¸ y, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸.
    """
    print("[INFO] ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°...")

    X = train.drop(['sk_id_curr', 'target'], axis=1)
    y = train["target"].astype(int)

    # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° 'Y'/'N'
    binary_cols = ["flag_own_car", "flag_own_realty"]
    for col in binary_cols:
        if col in X.columns:
            X[col] = X[col].replace({'Y': 1, 'N': 0})

    return X, y

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_preprocessing_pipeline(X):
    """
    Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ColumnTransformer Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğ¼Ğ¸, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸.
    """
    binary_cols = [col for col in X.columns if X[col].nunique() == 2 and X[col].dtype in ['int', 'float']]

    num_cols = [col for col in X.select_dtypes(include=['int', 'float']).columns
                if col not in binary_cols]

    cat_cols = X.select_dtypes(include='object').columns.tolist()
    if cat_cols:
        X[cat_cols] = X[cat_cols].astype(str)

    preprocessor = ColumnTransformer([
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),
        ("bin", "passthrough", binary_cols)
    ])

    return preprocessor

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ SHAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def select_top_shap_features(X, y, top_n=100):
    """
    ĞĞ±ÑƒÑ‡Ğ°ĞµÑ‚ LogisticRegression Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ¿-N Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ SHAP Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸
    """
    print("[INFO] Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ SHAP...")

    preprocessor = build_preprocessing_pipeline(X)
    model = LogisticRegression(random_state=RANDOM_STATE)
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Ğ”ĞµĞ»Ğ¸Ğ¼ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸
    X_train, X_valid, y_train, y_valid = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE
    )

    pipeline.fit(X_train, y_train)
    X_train_proc = pipeline.named_steps['preprocessor'].transform(X_train)
    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()

    explainer = shap.LinearExplainer(pipeline.named_steps["model"], X_train_proc)
    shap_values = explainer.shap_values(X_train_proc)

    shap_importances = pd.Series(
        np.abs(shap_values).mean(axis=0), index=feature_names
    ).sort_values(ascending=False)

    top_features = shap_importances.head(top_n).index.tolist()
    print(f"[INFO] Ğ¢Ğ¾Ğ¿-{top_n} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾.")
    return top_features

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Ğ“Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def optimize_and_evaluate(X, y, selected_features):
    """
    ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Optuna Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ Ğ´ĞµÑ€ĞµĞ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹
    Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ñ‡Ğ°Ñ….
    """
    X = X[selected_features].copy()

    def objective_logreg(trial):
        C = trial.suggest_float('C', 1e-4, 1e2, log=True)
        solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs'])
        max_iter = trial.suggest_int('max_iter', 100, 1000)

        preprocessor = build_preprocessing_pipeline(X)
        model = LogisticRegression(C=C, solver=solver, max_iter=max_iter, random_state=42)
        pipe = Pipeline([('preprocessor', preprocessor), ('model', model)])
        score = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc', n_jobs=-1)
        return -np.mean(score)

    def objective_dt(trial):
        max_depth = trial.suggest_int('max_depth', 2, 20)
        min_samples_split = trial.suggest_int('min_samples_split', 2, 100)
        min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 100)
        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])
        criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])

        preprocessor = build_preprocessing_pipeline(X)
        model = DecisionTreeClassifier(
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            criterion=criterion,
            random_state=42
        )
        pipe = Pipeline([('preprocessor', preprocessor), ('model', model)])
        score = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc', n_jobs=-1)
        return -np.mean(score)

    optuna.logging.set_verbosity(optuna.logging.WARNING)

    # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ñ€ĞµĞ³
    print("[INFO] Optuna: Logistic Regression")
    study_lr = optuna.create_study(direction='minimize')
    study_lr.optimize(objective_logreg, n_trials=20, n_jobs=2)

    print("[INFO] Optuna: Decision Tree")
    study_dt = optuna.create_study(direction='minimize')
    study_dt.optimize(objective_dt, n_trials=20, n_jobs=2)

    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»Ñ
    best_lr = LogisticRegression(**study_lr.best_params, random_state=42)
    best_dt = DecisionTreeClassifier(**study_dt.best_params, random_state=42)
    preproc = build_preprocessing_pipeline(X)

    pipe_lr = Pipeline([('preprocessor', preproc), ('model', best_lr)])
    pipe_dt = Pipeline([('preprocessor', preproc), ('model', best_dt)])

    score_lr = cross_val_score(pipe_lr, X, y, cv=5, scoring='roc_auc')
    score_dt = cross_val_score(pipe_dt, X, y, cv=5, scoring='roc_auc')

    print(f"LogReg AUC:  {np.mean(score_lr):.4f}")
    print(f"DecTree AUC: {np.mean(score_dt):.4f}")

    best_model = pipe_dt if np.mean(score_dt) > np.mean(score_lr) else pipe_lr
    model_name = "decision_tree" if best_model is pipe_dt else "logreg"

    with open(f"src/app/modelling/final_model_{model_name}.pkl", "wb") as f:
        pickle.dump(best_model, f)

    print(f"[âœ…] Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° ĞºĞ°Ğº final_model_{model_name}.pkl")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Ğ¢Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
def main():
    start = time.time()

    parser = argparse.ArgumentParser(
        description="ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ SHAP Ğ¸ Optuna"
    )
    parser.add_argument("--application_file", required=True,
                        help="Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ application_train_test")
    parser.add_argument("--bureau_balance_file", required=True,
                        help="Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ bureau_balance")
    parser.add_argument("--output_file", required=True,
                        help="ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ SHAP Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹")

    args = parser.parse_args()

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    train, _ = load_and_merge_data(args.application_file, args.bureau_balance_file)

    # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    X, y = preprocess_data(train)

    # SHAP Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ â†’ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸
    selected_features = select_top_shap_features(X, y, top_n=100)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ SHAP Ñ„Ğ¸Ñ‡Ğ¸
    pd.DataFrame({'feature': selected_features}).to_csv(args.output_file, index=False)
    print(f"[ğŸ“ˆ] SHAP Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {args.output_file}")

    # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
    optimize_and_evaluate(X, y, selected_features)

    print(f"[â±] Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ: {time.time() - start:.2f} ÑĞµĞº.")
